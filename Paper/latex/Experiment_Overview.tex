\section{Experimental Overview}

In the next few sections of the paper, we will give detailed explanations of the experimental tasks we consider. As a preview, the tasks we consider are:

\begin{figure*}[t]
	\centering
	\resizebox{!}{3.5in}{
	   \input{img/tweet_complex_object}
	 }
	\caption{Object relationship and frequency of Tweet Objects (for one million tweets)}
	\label{fig:tweet_complex_object}
\end{figure*}

\begin{enumerate}
	\item A set of serialized objects stored externally on an HDD; the task is to read the objects into memory and deserialize them to their in-memory representation.
	\item A set of objects are stored in a large file (larger than the available RAM). The task is to perform an external sort of the file in order to perform a duplicate removal.
	\item A set of objects are partitioned across a number of machines in a network; the task is to send requests to the 	machines. Each machine answers the request by serializing the objects, then sending them over the network to the requesting machine.
	\item Finally, a set of sparse vectors are stored across various machines on a network. The task is to perform a tree aggregation where the vectors are aggregated over $log ( n )$
	hops.
\end{enumerate}

\subsection{Twitter Data Set}
For the various experiments, we use twitter data sets \cite{tweet_objects}, implemented using each of the ten different physical implementations. 

\subsection{Encoding sizes}
The ten different complex object implementations that we considered have very different encoding densities when the objects are serialized for storage or transmission across the network. The average, per-object sizes are given in Table %\ref{tbl:object_size}. 
%\begin{table}
%	\centering
%	\caption{Frequency of some Tweet Objects  (for 1 million tweets) }
%	\label{tbl:object_size}
%	\begin{adjustbox}{width=\columnwidth,center}	
		
%		\begin{tabular}{|c|c|c|} \hline
%			Object Name &Parent Object &Frequency\\ \hline
%			tweet  & root object& 1,000,000 \\ \hline
%			users & tweet & 1,000,000 \\ \hline
%			coordinates  &tweet& 1586 \\ \hline
%			place & tweet & 11974 \\ \hline
%			quoted status  & tweet & 177537 \\ \hline
%			retweeted status  & tweet & 598517 \\ \hline
%			entities  & tweet & 1,000,000 \\ \hline
%			extended entities  & tweet & 51479 \\ \hline
%			hashtags  & entities & 398720 \\ \hline
%			media  & entities & 51481 \\ \hline
%			urls  & entities & 417176 \\ \hline
%			user mentions  & entities & 1063216 \\ \hline
%			symbols  & entities & 7793 \\ \hline
%			sizes  & media & 7793 \\ \hline
%			media sizes  & sizes & 51485 \\ \hline
%			thumb  & media sizes & 51485 \\ \hline
%			large  & media sizes & 51485 \\ \hline
%			medium  & media sizes & 51485 \\ \hline
%			small  & media sizes & 51485 \\ \hline
%			\hline\end{tabular}
%	\end{adjustbox}
%\end{table}

\begin{table}
	\centering
	\caption{tweet complexity }
	\label{tbl:object_size}
	\begin{adjustbox}{width=\columnwidth,center}	
		
		\begin{tabular}{|c|c|} \hline
			Tweet type & Frequency\\ \hline
			Simple tweets(retweet \& quote are null ) & 332,901\\ \hline
			Retweets & 489,562\\ \hline
			Quote & 68,582\\ \hline
			Retweet \& Quote & 108,955\\ \hline
			Total & 1,000,000 \\ \hline
			
			\hline\end{tabular}
	\end{adjustbox}
\end{table}

\begin{table}
	\centering
	\caption{Comparison of object size for 1 million tweet }
	\label{tbl:object_size}
	\begin{adjustbox}{width=\columnwidth,center}	
		
		\begin{tabular}{|l|c|c|} \hline
		 \textbf{Serialization Methods} & \textbf{Serialized file size(gigabyte)}\\ \hline
			Java Default  & 4.6 \\ \hline	
			Java Json Gzip  & 1.4 \\ \hline	
			Java Bson  & 4.9 \\ \hline	
			Java Protocol Buffer  & 1.9 \\ \hline	
			Java Kyro  & 1.9 \\ \hline	
			Java Hand Coded ByteBuffer  & 2.3 \\ \hline	
			Java FaltBuffers  & 2.9 \\ \hline	
			C++ Hand Coded  & 2.1 \\ \hline	
			C++ InPlace  & 3.2 \\ \hline	
			C++ Boost  & 2.2 \\ \hline	
			C++ Protocol Buffer  & 1.9 \\ \hline
			C++ Bson  & 4.6 \\ \hline	
			C++ FaltBuffers  & 2.9 \\ \hline	
			Rust Json  & 4.8 \\ \hline
			Rust Bincode  & 2.4 \\ \hline			
			Rust MessagePack  & 1.9 \\ \hline			
			Rust Bson  & 4.5 \\ \hline			
			Rust FlexBuffers  & 4.3 \\ 						
			\hline\end{tabular}
	\end{adjustbox}
\end{table}

\begin{table}
	\centering
	\caption{Lines of code for serialization and de-serialization  }
	\label{tbl:object_size}
	\begin{adjustbox}{width=\columnwidth,center}	
		
		\begin{tabular}{|l|c|c|} \hline
			\textbf{Serialization Methods} & \textbf{Serialize} &  \textbf{De-Serialize}   \\ \hline
			Java Default  & 4  & 4 \\ \hline	
			Java Json Gzip  & 2 & 4\\ \hline	
			Java Bson  & 50  & 120 \\ \hline	
			Java Protocol Buffer  & 200 & 1 \\ 
			                      &\textcolor{red}{with 20 extra files} &  \\ \hline	
			Java Kyro  & 40 & 40 \\ \hline	
			Java Hand Coded ByteBuffer  & 150 & 150 \\ \hline	
			Java FaltBuffers  & 250 & 1 \\ 
							  &\textcolor{red}{with 42 extra files} &  \\ \hline	
			C++ Hand Coded  & 70  & 100 \\ \hline	
			C++ InPlace  & 80 & 1 \\ \hline	
			C++ Boost  & 1 & 2 \\ \hline	
			C++ Protocol Buffer  & 200 & 1\\ 
									&\textcolor{red}{with 20 extra files} &  \\ \hline	
			C++ Bson  & 40 & 100 \\ \hline	
			C++ FaltBuffers  & 250  & 1 \\ 
									&\textcolor{red}{with 42 extra files} &  \\ 	\hline	
			Rust Json  & 1 & 1 \\ \hline
			Rust Bincode  & 1 & 1\\ \hline			
			Rust MessagePack  & 1 & 1\\ \hline			
			Rust Bson  & 1 & 1\\ \hline			
			Rust FlexBuffers  & 1 & 1\\						
			\hline\end{tabular}
	\end{adjustbox}
\end{table}




\subsection{Experimental Details}
We run our experiments on Google Cloud costume instances which have 4 vCPU cores, 32 GB RAM and 3000 GB standard persistent disk(Sustained random IOPS limit: read=2,250 and write=4,500) running with Ubuntu Ubuntu 18.04.4 LTS. Before running each experiment task, we "warmed up" the Java Garbage Collector (GC) by creating a large number of objects. We do not include this warm-up-time in our performance time calculations.

We used two Java GC flags $-XX:-UseGCOverheadLimit$ and $-XX:+UseConcMarkSweepGC$. The first flag is used to avoid OutOfMemoryError exceptions while using the complete RAM size for data processing and the second flag is for running concurrent garbage collection.

We run all of our experiments 3 times and observed that the results have low variance. In this paper we present the average of those runs. Before running each experiment, we deleted th OS cache using the Linux command: $echo 3 > /proc/sys/vm/drop\_caches$.

Our Java implementation is written using Java 8 with the Oracle JDK version "1.8.0\_241" and for our C++ implementation we use the C++11, compiled using clang++ (version 6.0.0).

The source codes of our implementation and a brief description of technical details can be found on the Github Repository \footnote{The source code of our Implementation is available at \url{https://github.com/fathollahzadeh/serialization}} .
