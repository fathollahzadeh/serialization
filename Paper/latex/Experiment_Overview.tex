\section{Experimental Overview}

In the next few sections of the paper, we will give detailed explanations of the experimental tasks we consider. As a preview, the tasks we consider are:

\begin{enumerate}
	\item A set of serialized objects stored externally on an HDD; the task is to read the objects into memory and deserialize them to their in-memory representation.
	\item A set of objects are stored in a large file (larger than the available RAM). The task is to perform an external sort of the file in order to perform a duplicate removal.
	\item A set of objects are partitioned across a number of machines in a network; the task is to send requests to the 	machines. Each machine answers the request by serializing the objects, then sending them over the network to the requesting machine.
	\item Finally, a set of sparse vectors are stored across various machines on a network. The task is to perform a tree aggregation where the vectors are aggregated over $log ( n )$
	hops.
\end{enumerate}

\subsection{Twitter Data Set}
For the various experiments, we use twitter data sets \cite{tweet_objects}, implemented using each of the ten different physical implementations. 

\subsection{Encoding sizes}
The ten different complex object implementations that we considered have very different encoding densities when the objects are serialized for storage or transmission across the network. The average, per-object sizes are given in Table \ref{tbl:object_size}. 
\begin{table}
	\centering
	\caption{Frequency of some Tweet Objects  (for 1 million tweets) }
	\label{tbl:object_size}
	\begin{adjustbox}{width=\columnwidth,center}	
		
		\begin{tabular}{|c|c|c|} \hline
			Object Name &Parent Object &Frequency\\ \hline
			tweet  & root object& 1,000,000 \\ \hline
			users & tweet & 1,000,000 \\ \hline
			coordinates  &tweet& 1586 \\ \hline
			place & tweet & 11974 \\ \hline
			quoted status  & tweet & 177537 \\ \hline
			retweeted status  & tweet & 598517 \\ \hline
			entities  & tweet & 1,000,000 \\ \hline
			extended entities  & tweet & 51479 \\ \hline
			hashtags  & entities & 398720 \\ \hline
			media  & entities & 51481 \\ \hline
			urls  & entities & 417176 \\ \hline
			user mentions  & entities & 1063216 \\ \hline
			symbols  & entities & 7793 \\ \hline
			sizes  & media & 7793 \\ \hline
			media sizes  & sizes & 51485 \\ \hline
			thumb  & media sizes & 51485 \\ \hline
			large  & media sizes & 51485 \\ \hline
			medium  & media sizes & 51485 \\ \hline
			small  & media sizes & 51485 \\ \hline
			\hline\end{tabular}
	\end{adjustbox}
\end{table}

\begin{table}
	\centering
	\caption{tweet complexity }
	\label{tbl:object_size}
	\begin{adjustbox}{width=\columnwidth,center}	
		
		\begin{tabular}{|c|c|} \hline
			Tweet type & Frequency\\ \hline
			Simple tweets(retweet \& quote are null ) & 332,901\\ \hline
			Retweets & 489,562\\ \hline
			Quote & 68,582\\ \hline
			Retweet \& Quote & 108,955\\ \hline
			Total & 1,000,000 \\ \hline
			
			\hline\end{tabular}
	\end{adjustbox}
\end{table}

\begin{table}
	\centering
	\caption{Comparison of object size for 1 million tweet }
	\label{tbl:object_size}
	\begin{adjustbox}{width=\columnwidth,center}	
		
		\begin{tabular}{|l|c|c|} \hline
		 \textbf{Serialization Methods} & \textbf{Serialized file size(gigabyte)}\\ \hline
			Java Default  & 4.6 \\ \hline	
			Java JSON  & 4.3 \\ \hline	
			Java BSON  & 4.6 \\ \hline	
			Java Protocol Buffer  & 1.9 \\ \hline	
			Java Kyro  & 1.9 \\ \hline	
			Java Hand Coded ByteBuffer  & 2.3 \\ \hline	
			C++ Hand Coded  & 2.1 \\ \hline	
			C++ InPlace  & 3.2 \\ \hline	
			C++ Boost  & 2.2 \\ \hline	
			C++ Protocol Buffer  & 1.9 \\ \hline					
			\hline\end{tabular}
	\end{adjustbox}
\end{table}


\subsection{Experimental Details}
We run our experiments on a virtual server instances which have 8 vCPU cores, 32 GB RAM and one 800
GB hard disks (Network Instance Store) running with Ubuntu 18.04.3 LTS. Before running each experiment task, we “warmed up” the Java Garbage Collector (GC) by creating a large number of objects. We do not include this warm-up-time in our performance time calculations.

We used two Java GC flags $-XX:-UseGCOverheadLimit$ and $-XX:+UseConcMarkSweepGC$. The first flag is used to avoid OutOfMemoryError exceptions while using the complete RAM size for data processing and the second flag is for running concurrent garbage collection.

We run all of our experiments 5 times and observed that the results have low variance. In this paper we present the average of those runs. Before running each experiment, we
deleted th OS cache using the Linux command: $echo 3 > /proc/sys/vm/drop_caches$.

Our Java implementation is written using Java 8 with the Oracle JDK version "1.8.0\_241" and for our C++ implementation we use the C++11, compiled using clang++ (version 6.0.0).

The source codes of our implementation and a brief description of technical details can be found on the Github Repository \footnote{The source code of our Implementation is available at \url{https://github.com/fathollahzadeh/serialization}} .
